# Python爬虫 - 03.requests模块进阶

## **1.多线程**

```py
"""
多线程
异步
"""

"""
进程：资源单位 ( 公司 )。每一个进程至少要有一个线程

线程：执行单位 (员工干活)
"""

# 启动每一个程序默认都会有一个主线程


# def func():
#     for i in range(1000):
#         print("func", i)
#
#
# if __name__ == '__main__':
#     print("您好呀")
#     func()
#     for i in range(1000):
#         print("main", i)


# # 多线程 第一套写法
# from threading import Thread  # 导入线程类
#
#
# def func():
#     for i in range(1000):
#         print("func", i)
#
#
# if __name__ == '__main__':
#     t = Thread(target=func)  # 线程分配目标任务
#     t.start()  # 开始执行该线程。多线程状态为可以工作状态，具体的执行时间由CPU决定
#
#     for i in range(1000):
#         print("main", i)


# 多线程 第2套写法 ,用的比较多
from threading import Thread  # 导入线程类


class MyThread(Thread):  # 类 继承
    def run(self):  # 固定的  -> 当线程被执行的时候，被执行的就是run()
        for i in range(1000):
            print("子线程", i)


if __name__ == '__main__':

    t = MyThread()
    t.start()

    for i in range(1000):
        print("主线程", i)

```


## **2.多进程**

```py
"""
多进程比较耗费资源，不是很建议
"""
# 导包
from multiprocessing import Process


def func():
    for i in range(1000):
        print("子进程", i)


if __name__ == '__main__':
    p = Process(target=func)
    p.start()

    for i in range(1000):
        print("主进程", i)

```

## **3.线程池**

```py
"""
# 线程池 ：一次性开辟一些线程，我们用户直接给线程池子提交任务。线程任务的调度交给线程池来完成


"""

from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor


def fn(name):
    for i in range(1000):
        print(name, i)


if __name__ == '__main__':
    # 创建线程池
    with ThreadPoolExecutor(50) as t:  # 创建一个有50个线程组成的线程池
        # 准备多少个任务
        for i in range(100):
            t.submit(fn, name=f"线程{i}")

    # 等待线程池中的任务全部执行完毕，才继续执行( 守护 )
    print("123")

```

## **4.线程池实战**

```py

"""
# 1.如何提取单个页面的数据
# 2.上线程池，多个页面同时抓取
"""
import requests
from lxml import etree
import csv
from concurrent.futures import ThreadPoolExecutor

f = open("data.csv", mode="w", encoding="utf-8")
csvWrite = csv.writer(f)


def download_one_page(url):
    # 拿到页面源代码
    resp = requests.get(url)
    html = etree.HTML(resp.text)
    table = html.xpath("/html/body/div[2]/div[4]/div[1]/table")[0]
    trs = table.xpath("./tr[position()>1])")
    # 拿到每个tr
    for tr in trs:
        tr.xpath("./td/text()")
        # 对数据做一个简单的处理：\\ /去掉  生成器
        txt = (item.replace("\\", '').replace("/", "") for item in txt)
        # 写数据到文件中
        csvWrite.writerow(txt)

    print("url提取完毕")
    f.close()


if __name__ == '__main__':
    with ThreadPoolExecutor(50) as t:  # 上50个线程
        for i in range(1, 200):  # 循环多少页
            t.submit(download_one_page, f"url链接带有页面的")
    print("全部下载完毕!")
    pass

```

## **5.多线程代理ip**

```py
"""
通过多线程方式获取IP代理池
目标： https://www.kuaidaili.com/free/inha/2/
# 思路：
1.提取单页面数据,并验证有效性，有效的保留
2.上线程池，多页面同时抓取
"""

import requests
from lxml import etree
import csv
from concurrent.futures import ThreadPoolExecutor
import time

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"
}

f = open("ip代理.csv", mode='w', encoding="utf-8")
csw_write = csv.writer(f)
csw_write.writerow(["有效IP代理"])


def down_onepage_ip(url):
    resp = requests.get(url, headers=headers, verify=False)
    html = etree.HTML(resp.text)
    trs = html.xpath("/html/body/div[2]/section[1]/div/div/div/div[1]/div/article/div[2]/div/figure/table/tbody/tr")

    for tr in trs:
        ipList = tr.xpath("./td/text()")
        ip = ipList[0]
        port = ipList[1]
        httpType = str.lower(ipList[4])
        httpPorx = f'{httpType}://{ip}:{port}'
        check_proxie_url = "http://httpbin.org/ip"
        proxie = {
            httpType: httpPorx
        }
        try:
            resprox = requests.get(check_proxie_url, headers=headers, proxies=proxie, timeout=10)
            csw_write.writerow([httpPorx])
            print(f"==> 代理可用，IP为\t {httpPorx}")
        except Exception:
            print(f"请求失败，代理IP无效！ ")

    print(f"===> 当前 {url.split('/')[-1]}页 IP页面数据提取完毕 ")


if __name__ == '__main__':
    # down_onepage_ip("https://www.beesproxy.com/free/page/1")
    # check_ip("http://www.baidu.com")
    # 创建线程池里面有多少个线程
    with ThreadPoolExecutor(25) as t:
        # 多少个页面循环
        for i in range(20):
            t.submit(down_onepage_ip, f"https://www.beesproxy.com/free/page/{i}")

    print("===> 全部提取完毕")
    f.close()

```

## **6.协程**

```py
"""
协程
"""

import time


def func():
    print("我爱黎明")
    time.sleep(3)  # 让当前的线程处于阻塞状态，CPU是不为我工作的
    print("我真的爱黎明")


if __name__ == '__main__':
    func()

# input() 程序也是处于阻塞状态
# requestes.get(url) 在网络请求返回数据之前，程序也是处于阻塞状态的
# 一般情况下，当程序处理IO操作时候，线程都会处于阻塞状态

# 协程：当程序遇见了IO操作时候，可以选择性的切换到其他任务上
# 在微观上 是一个任务一个任务的进行切换，切换条件一般就是IO操作
# 在宏观上，我们能看到的其实是多个任务一起在执行
# 多任务异步操作

# 上方所讲的一切，都是在单线程的条件下


```

## **7.python编写协程程序**

```py
"""
python编写协程的程序
市面上有四种方式，这里只学习一种比较常见的方式
"""

import asyncio

# # 异步
# async def func():
#     print("你好呀，我叫赛利亚")
#
#
# if __name__ == '__main__':
#     g = func()  # 此时的函数是异步协程函数，此时函数执行得到的是一个协程对象
#     asyncio.run(g)  # 协程程序运行需要asyncio模块的支持
#     # print(g)

import time


#
# async def func1():
#     print("你好呀，我叫赛利亚")
#     # time.sleep(3)  # 当程序出现了同步操作的时候，异步就中断了
#     await asyncio.sleep(3)  # 异步操作的代码
#     print("你好呀，我叫赛利亚")
#
#
# async def func2():
#     print("你好呀，我叫赛利亚2")
#     # time.sleep(3)
#     await asyncio.sleep(3)  # 异步操作的代码
#     print("你好呀，我叫赛利亚2")
#
#
# async def func3():
#     print("你好呀，我叫赛利亚3")
#     # time.sleep(3)
#     await asyncio.sleep(3)  # 异步操作的代码 ，await 挂起
#     print("你好呀，我叫赛利亚4")
#
#
# if __name__ == '__main__':
#     f1 = func1()
#     f2 = func2()
#     f3 = func3()
#     tasks = [
#         f1, f2, f3
#     ]
#     t1 = time.time()
#     # 一次性启动多个任务（协程）
#     asyncio.run(asyncio.wait(tasks))
#     t2 = time.time()

# print(t2 - t1)

# =====================下面是 官方推荐操作方式

# async def func1():
#     print("你好呀，我叫赛利亚")
#     await asyncio.sleep(3)  # 异步操作的代码
#     print("你好呀，我叫赛利亚")
#
#
# async def func2():
#     print("你好呀，我叫赛利亚2")
#     await asyncio.sleep(3)  # 异步操作的代码
#     print("你好呀，我叫赛利亚2")
#
#
# async def func3():
#     print("你好呀，我叫赛利亚3")
#     await asyncio.sleep(3)  # 异步操作的代码 ，await 挂起
#     print("你好呀，我叫赛利亚4")

#
# async def main():
#     # 第一种写法 不是很推荐
#     # f1 = func1()
#     # await f1 # 一般await挂起操作放在协程对象前面
#
#     # 第二种写法 （推荐）
#     tasks = [
#         asyncio.create_task(func1()), # 创建一个协程任务 py3.8以后用这种方法
#         asyncio.create_task(func2()),
#         asyncio.create_task(func3())
#      ]
#     await asyncio.wait(tasks)
#
#
# if __name__ == '__main__':
#     asyncio.run(main())


# ============协程模拟爬虫下载视频 =====

# 在爬虫领域的应用

async def download(url):
    print("准备下载...")
    await asyncio.sleep(2)  # 模拟的是网络请求， requests.get()这个是同步操作 要注意
    print("下载完成")


async def main():
    urls = [
        "http://www.baidu.com",
        "http://bilibili.com",
        "http://www.163.com"
    ]

    tasks = []
    for url in urls:
        d = asyncio.create_task(download(url))  # 创建协程对象
        tasks.append(d)  # 将协程对象 添加在创建的列表任务里面
    await asyncio.wait(tasks)  # 将任务列表 一起去跑

    # tasks = [asyncio.create_task(download(url)) for url in urls]  # 这么干也是可以的
    
if __name__ == '__main__':
    asyncio.run(main())  # 运行协程

```

## **8.协程(异步)aiohttp模块**

> **当前python的环境是 python 3.10

```py
"""
异步的 http操作
requests.get() 是同步的代码  ==> 异步操作的 需要用 aiohttp异步
目标网站
http://umei.cc
"""

# 导包 异步操作模块
import asyncio
# 导包 异步http请求操作模块
import aiohttp

urls = [
    # https 会报错，暂时没学到解决办法
    "http://img.netbian.com/file/2022/0805/233217zUWmF.jpg",
    "http://img.netbian.com/file/2022/1031/234333kApFn.jpg",
    "http://img.netbian.com/file/2022/0127/235536d7bRL.jpg"
]


# 异步协程 下载三张图片
async def aiodownload(url):
    name = url.rsplit('/', 1)[1]  # 从右侧分割
    # 发送请求
    # 得到图片内容
    # 保存到文件
    # s = aiohttp.ClientSession()   # 等价于 requests模块
    # with 相当于文件操作，会自动close
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            # 请求回来了，写入文件
            # 可以自己去学习一个模块,aiofiles 异步模块
            with open("异步下载的图片/" + name, mode='wb') as f:
                f.write(await resp.content.read())  # 读取内容是异步的，需要await挂起

    print("success!", name)
    # resp.content.read() # 读取内容 等价于 resp.content


async def main():
    tasks = []
    for url in urls:
        tasks.append(asyncio.create_task(aiodownload(url)))

    await asyncio.wait(tasks)


if __name__ == '__main__':
    asyncio.run(main())

```






## **9.**

