# Python爬虫

---

## **章节目录**

> [!WARNING|label:本章需要学习的内容]


# 1.爬虫概念
![1](https://p.ananas.chaoxing.com/star3/origin/31cdc8354bb1dd433724c42380f86533.png)
![2](https://p.ananas.chaoxing.com/star3/origin/1bbc75849c13f74f34f9f05713ce637f.png)

 
        
- 微信的公众号内的文章 有版权的，付费的等 都是不能爬的 ，速度也不能太快，违法违规的

- 反爬机制
    - 是作用到门户网站中。如果网站不香让爬虫轻易爬到数据，它可以指定相关的机制或者措施阻止爬虫程序爬取其数据
- 反反爬策略
    - 是作用在爬虫程序中。我们爬虫可以定制相关的策略破击反爬机制从而爬取到相关的数据。

- **课程第一个反爬机制：**    
    - robots协议：防君子不防小人
        - 是一个纯文本的协议，协议中规定了该网站哪些数据可以被哪些爬虫爬取，哪些不可以被爬取。
    - 破解：
        - 你自己主观性的不遵从该协议即可。

- **爬虫的价值：**
    - 实际应用
    - 就业

- 爬虫究竟是合法还是违法？
    - 在法律中是不被禁止的
    - 具有违法风险
    - 善意爬虫  恶意爬虫     

- 爬虫带来的风险可以体现在如下2方面：
    - 爬虫干扰了被访问网站的正常运营
    - 爬虫抓取了收到法律保护的特定类型的数据或信息

- 如何在使用编写爬虫的过程中避免进入局子的厄运呢？
    - 时常的优化自己的程序，避免干扰被访问网站的正常运行
    - 在使用，传播爬取到的数据时，审查抓取到的内容，如果发现了涉及到用户因此商业机密等敏感内容需要及时停止爬取或传播



<hr>



# **3.第一个爬虫程序**

```python
# 第一个爬虫程序
# 百度
# 需求：用程序模拟浏览器，输入一个网址，从该网址中获取到资源或者内容
# Python搞定以上需求，特别简单

# 导包
from urllib.request import urlopen

url = "http://www.baidu.com"
resp = urlopen(url)

with open("baidu.html", 'w', encoding="utf-8") as f:
    f.write(resp.read().decode('utf-8'))

print("over")




```

<hr>

# **4.web请求过程剖析**

```
# 1.服务器渲染:在服务器那边直接把数据和html整合在起。 统一返回给浏览器
# 在 页面源代码中能看到数据
# 2.客户端渲染:|
# 第一次请求只要一个html骨架。第二次请求拿到数据。进行数据展示。 
# 在页面源代码中， 看不到数据
# 熟练使用浏览器抓包工具
```

<hr>

 # **5.HTTP协议**

 
协议:就是两个计算机之间为了能够流畅的进行沟通而设置的一个君子协定.常见的协议有TCP/IP. SOAP协议，HTTP
协议，SMTP协议等等.....

HTTP协议，Hyper Text Transfer Protocol (超文本传输协议)的缩写，是用于从万维网(WWW:World Wide Web )服
务器传输超文本到本地浏览器的传送协议.直白点儿，就是浏览器和服务器之间的数据交互遵守的就是HTTP协议.

HTTP协议把一条消息分为三大块内容.无论是请求还是响应都是三块内容

请求:
```
 1、请求行->请求方式(get/post) 请求url地址协议
 2、请求头( Request Headers )->放一些服务器要使用的附加信息 
    
 3、
 4、请求体->一般放一些请求参数
```


响应:
```
1状态行->协议状态码
2响应头 ( Response Headers )->放一些客户端要使用的一些附加信息
3
响应体-> 服务器返回的真正客户端要用的内容(HTML，json)等
 ```
在后面我们写爬虫的时候要格外注意请求头和响应头.这两个地方-般都隐含着一些比较重要的内容

![1](https://p.ananas.chaoxing.com/star3/origin/d75cbdbdd923cc9ef863417d27d68863.png)

- **请求头中最常见的一些重要内容(爬虫需要):**

    - 1. User-Agent:请求载体的身份标识(用啥发送的请求)
    - 2. Referer: 防盗链(这次请求是从哪个页面来的?反爬会用到
    - 3. cookie: 本地字符串数据信息(用户登录信息，反爬的token)

- **响应头中一些重要的内容:**

    - 1.cookie:本地字符串数据信息(用户登录信息，反爬的token)
    - 2.各种神奇的莫名其妙的字符串(这个需要经验了，一般都是token字样，防止各种攻击和反爬)

- **请求方式**

    - GET :  显示提交
    - POST : 隐式提交            

<hr>

# **6.requests模块入门**



##  安装requests模块
- 不是python自带的模块，需要安装
```py
pip install requests
```

- 使用pycharm的Terminal安装

```py
打开pycharm终端，输入
pip install requests 
然后回车 
```

- pip清华源 国内速度会比较快

https://mirrors.tuna.tsinghua.edu.cn/help/pypi/

``` py
国内清华源下载安装模块 requests

pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  requests

```

- **判断自己是否安装过requests模快**
    - 可以pycharm输入代码 import requests 看是否报错
    - pycharm 插件里面看是否已经安装

## requests入门案例1：模拟搜狗浏览器搜索

```python
"""
resuests模块入门
入门案例1.模拟搜狗浏览器 搜索
"""
import requests

query = input("请输入一个你喜欢的明星名字：")
url = f"https://sogou.com/web?query={query}"

headers_dic = {
    # 模拟请求头，用户UA浏览器设备
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"
}

resp = requests.get(url, headers=headers_dic)  # 返回响应
# print(resp) 请求返回结果 <Response [200]>
print(resp)
print(resp.text)  # 拿到页面源代码
# 关闭resp 如果不关掉，请求过多时候则会提示请求过多的报错 
resp.close()  

```

## requests入门案例2：获取百度翻译的结果

- 百度翻译官网： https://fanyi.baidu.com/

- 输入英文单词，获取对应和其联系词的中文翻译

```python
"""
requests 入门案例2；获取百度翻译的结果
"""
# 导入包
import requests

# 请求api
url = "https://fanyi.baidu.com/sug"

# 请求表单 ，发送的数据必须放在字典中 ，通过data参数进行传递
Input_Word = input("请输入您要翻译的英文单词：")

form_data_dict = {
    "kw": Input_Word
}

# 发送POST 方式请求
resp = requests.post(url, data=form_data_dict)
# print(resp.json())  # 将服务器返回的内容直接处理成json()  = > dict

data_list = resp.json()['data']

print("================翻译开始======================")

for item in data_list:
    print(f"联想词：{item['k']} \t 翻译为 : {item['v']}")

print("================翻译结束======================")
# 关闭resp 如果不关掉，请求过多时候则会提示请求过多的报错 
resp.close()  
```

## requests入门案例3：豆瓣电影分类排行榜

- 豆瓣分类排行榜  <br>
 https://movie.douban.com/typerank?type_name=喜剧&type=24&interval_id=100:90&action=

![喜剧](https://p.ananas.chaoxing.com/star3/origin/3c5ded44e02fbaa6bc2e452dd9e6aee7.png)

- 浏览器F12 获取network中，筛选 XHR (一般是AJAX请求)


```python
"""
requests请求入门案例3：豆瓣电影分类排行榜

"""
import requests

# 请求网址  https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action=&start=0&limit=20
# 1.请求网址太长了，要重新封装下
url = "https://movie.douban.com/j/chart/top_list"

# 2.重新封装参数
param = {
    "type": "24",
    "interval_id": "100:90",
    "action": "",
    "start": 0,
    "limit": 20
}
# 6.思考后，自己封装请求头，字典格式
headers_dict = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"
}

# 3.提交数据(不带请求头，返回空源码)
# get 方式的数据参数 叫 params
# post 方式的数据参数 叫 data
# resp = requests.get(url, params=param)

# 7.提交数据(带上自己封装的请求头，正常了 )
resp = requests.get(url, params=param, headers=headers_dict)

print(resp.request.url)  # 调试输出请求的网址

# 4.获取源码并转换为json ，输出为空， 应该是有反爬机制，首先考虑UA
print(resp.json())

# 5.思考：调试输出默认的请求头是什么
print(resp.request.headers)
# 调试输出默认请求头结果 ，不是浏览器的呀，要自己模拟浏览器才行    {'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
# 需要在 requests.get 请求前设置请求头

# 关闭resp 如果不关掉，请求过多时候则会提示请求过多的报错 
resp.close()  
```

# **补充**
```
# 请求后，需要在最后地方关闭到请求resp 
resp.close() # 关闭resp

# 如果不关掉，会提示请求过多的报错 ⭐️ 必须关闭
# 和 打开文件需要关闭 道理一样
```
 

