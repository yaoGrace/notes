# Python爬虫 -第2章节 数据解析与提取
 
--- 
 
# **1.数据解析概述**

在上一章中，我们基本上掌握了抓取整个网页的基本技能.但是呢，大多数情况下，我们并不需要整个网页的内容，只是需要那么一小部分.怎么办呢?这就涉及到了数据提取的问题.

- 本课程中，提供三种解析方式:
    - 1. re解析     (运行速度快)
    - 2. bs4解析    (比较简单，效率不高)
    - 3. xpath解析  (目前比较流行)

这三种方式可以混合进行使用，完全以结果做导向，只要能拿到你想要的数据.用什么方案并不重要.当你掌握了这些之后.再考虑性能的问题.

---

# **2.正则表达式**
 
Regular Expression,正则表达式，一种使用表达式的方式对字符串进行匹配的语法规则

我们抓取到的网页源代码本质上就是个超长的字符 串，想从里面提取内容.用正则再合适不过了.

## 正则优缺点

- 正则的优点:速度快，效率高，准确性高
- 正则的缺点:新手上手难度有点儿高.

不过只要掌握了正则编写的逻辑关系，写出一个提取页面内容的正则其实并不复杂

正则的语法:使用元字符进行排列组合用来匹配字符串

在线测试正则表达式 
- https://tool.oschina.net/regex/


元字符:具有固定含义的特殊符号

 
## 常用元字符 

```
.    匹配除换行符以外的任意字符
\W   匹配字母或数字或下划线
\S   匹配任意的空白符
\d   匹配数字
\n   匹配一个换行符
\t   匹配一个制表符

^   匹配字符串的开始
$   匹配字符串的结尾

\W  匹配非字母或数字或下划线
\D  匹配非数字
\S  匹配非空白符
a|b 匹配字符a或字符b
()  匹配括号内的表达式， 也表示一个组
[...]   匹配字符组中的字符 例如：[a-z0-9A-Z]
[^...]  匹配除了字符组中字符的所有字符
``` 

## 量词

量词：控制前面的元字符出现的次数 

```
*   重复零次或更多次
+   重复一次或更多次
？  重复零次或一次
{n} 重复n次
{n,}    重复n次或更多次
{n,m}    重复n到m次
```

贪婪匹配和惰性匹配
```
.*  贪婪匹配
.*? 惰性匹配
```

这两个要着重的说一下.因为我们写爬虫用的最多的就是这个惰性匹配.

---


# **3.re模块**

<iframe src="//player.bilibili.com/player.html?aid=553819904&bvid=BV1rv4y1K7yV&cid=711776183&page=21" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>


re模块

那么接下来的问题是，正则我会写了，怎么在python程序中使用正则呢?答案是re模块

re模块中我们只需要记住这么几个功能就足够我们使用了.

1. findall查找所有.返回list
```py
lst = re.findall("m", "mai le fo len, mai ni mei!")
print(lst) # ['m', 'm', 'm'] 
lst = re.findall(r"\d+", "5点之前，你要给我5000万") 
print(lst) # ['5'， ' 5000']
```

2. search会进行匹配.但是如果匹配到了第-个结果.就会返回这个结果.如果匹配不上search返回的则是None

```py
ret = re.search(r'\d'，'5点之前，你要给我5000万').group()
print(ret) # 5
```
3. match只能从字符串的开头进行匹配

```py
ret = re.match('a', ' abc').group()
print(ret) #a
```
4. finditer, 和findall差不多.只不过这时返回的是迭代器(重点)
```py
it = re.finditer("m", "mai le fo len, mai ni mei!")
for el in it:
    print(el.group)) #依然需要分组
```
5. compile( 可以将一个长长的正则进行预加载.方便后面的使用
```py
obj = re.compile(r'\d{3}') # 将正则表达式编译成为一个正则表达式对象，规则要匹配的是3个数字
ret = obj.search( ' abc123eeee') #正则表达式对象调用search, 参数为待匹配的字符串
print(ret.group()) #结果: 123
```

6.正则中的内容如何单独提取?

单独获取到正则中的具体内容可以给分组起名字
```py 
S = """
<div class= '西游记'><span id= ' 10010 '>中国联通</span></div>
"""
obj = re.compile(r"<span id='(?P<id> \d+)'>(?P<name> \w+)</span>", re.S)
result = obj. search(s)
print( result . group()) # 结果: <span id=' 10010'>中国联通</ span>
print(result. group("id")) #结果: 10010 #获取id组的内容
print(result . group("name")) #结果:中国联通#获取name组的内容
```
这里可以看到我们可以通过使用分组.来对正则匹配到的内容进一步 的进行筛选.


关于正则，还有一个重要的小点，也非常的简单，在本节中就不继续扩展了.下一小节的案例中会把这个小点进行简单的
介绍.

> **代码演示**

```python
"""
re模块
正则前面加一个r  只有好处，没坏处
"""
import re

# re.finall :匹配字符串中所有的符合正则的内容 ,返回的是list  (用的不多，效率不高)

lst = re.findall(r"\d+", "我的电话是10086，另外一个号码是123456")
print(lst)

print("====================================================")

# finditer :匹配字符串中所有的内容 [返回的是迭代器],从迭代器中拿到内容需要.group() 效率高 用的最多
it = re.finditer(r"\d+", "我的电话是10086，另外一个号码是123456")
for i in it:
    print(i.group())

print("====================================================")

# search 找到一个结果就返回(只匹配第一个)，返回的结果是Match对象，拿数据需要.group(),
s = re.search(r"\d+", "我的电话是10086，另外一个号码是123456")
print(s.group())

print("====================================================")
# match 从头开始匹配  (用的不是特别多)
m = re.match(r"\d+", "我的电话是10086，另外一个号码是123456")
print(s.group())

print("====================================================")
# 预加载正则表达式  好处：可以反复用 ，效率提高一点，代码好看
obj = re.compile(r"\d+")
ret = obj.finditer("我的电话是10086，另外一个号码是123456")
print(ret)
for i in ret:
    print(i.group())


print("====================================================")
# 取出正则里面的内容
s = """
    <div class="jay"><span id='1'>中国联通</span></div>
    <div class="jay5"><span id='3'>明星</span></div>
    <div class="jay4"><span id='2'>沈腾</span></div>
    <div class="jay3"><span id='4'>范思哲</span></div>
    <div class="jay2"><span id='5'>玉林</span></div>
    <div class="jay1"><span id='6'>翰林院</span></div>
"""

# re.S 即flag位置代码，表示 re.S 让.能匹配换行符
# (?P<分组名称>正则)  可以单独从正则匹配的内容中进一步取出内容
obj  = re.compile(r"<div class=\"(?P<CssClass>.*?)\"><span id='(?P<ids>\d+)'>(?P<BianLiang>.*?)</span></div>",re.S)
itRes =  obj.finditer(s)
for i in itRes:
    print(f"id = {i.group('ids')} ,CSS = {i.group('CssClass')}, 名称: {i.group('BianLiang')}")

```
---

# **4.手刃豆腐top250电影信息**


<iframe src="//player.bilibili.com/player.html?aid=553819904&bvid=BV1rv4y1K7yV&cid=711776404&page=23" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

> 代码演示

```python
"""
1. 拿到页面源代码  requests
2.通过re模块来提取想要的有效信息 re
3.文件存储在桐木岭下的 data.csv文件
"""

import requests
import re
import csv

# 获取源码
url = "https://movie.douban.com/top250"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15"
}
resp = requests.get(url,headers=headers)
page_content = resp.text   # 网页源码结果

# 解析数据 标题,评分，年份，评价人数 四个数据
regs = r'<li>.*?<div class="item">.*?<span class="title">(?P<title>.*?)</span>.*?<p class="">' \
       r'.*?<br>(?P<years>.*?)&nbsp;/.*?<span class="rating_num" property="v:average">(?P<Score>.*?)</span>' \
       r'.*?<span>(?P<Numbers>.*?)人评价</span>'
# 预处理
obj = re.compile(regs,re.S)
# 查询
result = obj.finditer(page_content)
# 文件处理
f = open('data.csv',mode='w',encoding='utf-8')

csvWrite = csv.writer(f)

# 遍历查询到的分组数据
for it in result:
    # print(i.group('title'))
    # print(i.group('years').strip())  #年份前面有空格，需要处理
    # print(i.group('Score'))
    # print(i.group('Numbers'))

    # 转换为字典
    dic = it.groupdict()
    # 处理去除字典中年份 前面的空格
    dic['years'] = dic['years'].strip()
    # 写入文件
    csvWrite.writerow(dic.values())

# 关闭文件，如果不关闭则会占用进程
f.close()

print('======== over ===========')

```

> 获取电影天堂电影信息案例

<iframe src="//player.bilibili.com/player.html?aid=553819904&bvid=BV1rv4y1K7yV&cid=711776441&page=25" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

---

- **电影天堂**
- 网址 http://dytt89.com


```python
"""
电影天堂案例：
http://dytt89.com
思路：
1.定位2022比看片
2.从2022比看片中提取到子页面的链接地址
3.请求子页面的链接地址，拿到我们想要的下载地址

"""

# 导包
import requests
import re
import csv

# get请求
domain = 'https://www.dytt89.com/'
resp = requests.get(domain, verify=False)  # verify=False 去掉安全验证
resp.encoding = 'gb2312'  # 指定字符集编码
page_content = resp.text  # 获取网页源码

# 提取2022比看片 拿到ul里面的li
obj1 = re.compile(r'2022必看热片</span>.*?<ul>(?P<ul>.*?)</ul>', re.S)
obj2 = re.compile(r"<a href='(?P<href>.*?)'", re.S)
obj3 = re.compile(r'◎片　　名　(?P<name>.*?)<br />.*?'
                  r'<td style="WORD-WRAP: break-word" '
                  r'bgcolor="#fdfddf"><a href="(?P<download>.*?)&', re.S)
result1 = obj1.finditer(page_content)

child_her_list = []
for i in result1:
    ul = i.group('ul')

    # 提取子页面链接：
    result2 = obj2.finditer(ul)
    for itt in result2:
        # 拼接子页面的url地址：域名+子页面地址
        child_href = domain + itt.group("href").strip('/')
        child_her_list.append(child_href)  # 把子页面链接保存起来
# 文件操作
f = open("电影天堂.csv",mode='w',encoding='utf-8')
csv_write = csv.writer(f)
# 提取子页面内容 循环取出
for href in child_her_list:
    # 获取子页面源码
    child_resp = requests.get(href, verify=False)
    child_resp.encoding = 'gb2312'

    # 提取数据
    result3 = obj3.search(child_resp.text)
    # 调试输出
    # print(f"片名【{result3.group('name')}】,下载地址 {result3.group('download')}")
    dic = result3.groupdict()
    # 写入csv数据文件内
    csv_write.writerow(dic.values())
    # break  # 测试

# 关闭文件
f.close()

# 结尾提示
print("========== 执行完毕 =========")
```

---

# **5.bs4解析-HTML语法**
 

bs4解析比较简单，但是呢，首先你需要了解-丢丢的html知识.然后再去使用bs4去提取，逻辑和编写难度就会非常简单和清晰

HTML (Hyper Text Markup Language)超文本标记语言，是我们编写网页的最基本也是最核心的一种语言.其语法规则
就是用不同的标签对网页上的内容进行标记，从而使网页显示出不同的展示效果.
```
<h1>
    我爱你
</h1>
```

上述代码的含义是在页面中显示"我爱你"三个字，但是我爱你三个字被```<h1>```和```</h1>```标记了.白话就是被括起来了.
被H1这个标签括起来了.这个时候.浏览器在展示的时候就会让我爱你变粗变大.俗称标题，所以HTML的语法就是用类
似这样的标签对页面内容进行标记.不同的标签表现出来的效果也是不一样的.
```
h1:-级标题
h2:二级标题
p:段落
font: 字体(被废弃了，但能用)
body:主体
```
这里只是给小白们简单科普-下， 其实HTML标签还有很多很多的.我们不需要一一列举(这是爬 虫课，不是前端课).

OK , 标签我们明白了，接下来就是属性了

```
<h1>
    我爱你
</h1>
<h1 align='right'>
    我爱你
</h1>  
```

有意思了.我们发现在标签中还可以给出xxx=xxx这样的东西.那么它又是什么呢?又该如何解读呢?


首先，这两个标签都是h1标签，都是一级标题， 但是下面这个会显示在右边.也就是说，通过xxx=xxx这种形式对h1标签

进一步的说明了.那么这种语法在html中被称为标签的属性.并且属性可以有很多个.例如:

```
<body text="green" bgcolor= "#eee">
你看我的颜色，贼健康
</body>
```

总结，html语法:
```
<标签属性="值" 属性="值">
    被标记的内容
</标签点>
```

有了这些知识，我们再去看bs4就会得心应手了.因为bs4就是通过标签和属性去定位页面上的内容的.

---

# **6.bs4解析-bs4模块安装和使用**

- 安装

```
pip install bs4 -i 清华源

或者 pycharm中插件里面安装
```

- 练习： 
- 北京新发地 北京最大的批发市场）菜价
- xinfadi.com.cn

- **思路**
    - 1.拿到源码
    - 2.使用bs4进行解析，拿到数据

```python
'''
- 练习：
- 北京新发地 北京最大的批发市场）菜价
- xinfadi.com.cn  这个案例用的是课堂代码，现在页面已经改变了，只学习思路即可

- **思路**
    - 1.拿到源码
    - 2.使用bs4进行解析，拿到数据
'''

import requests
from bs4 import BeautifulSoup
import csv

url = 'http://www.xinfadi.com.cn/priceDetail.html'
resp = requests.get(url)
# print(resp.text)

# 2.解析数据
#   1、页面源代码交给BeautifulSoup进行处理，生成bs对象
page = BeautifulSoup(resp.text, "html.parser")  # 指定html解析器
#   2、从bs 对象里面查找数据
#       find(标签,属性=值)       找一个
#       find_all(标签,属性=值)   找一堆

# table = page.find('tbody',class_="ul") # class是python的关键字，为了避免报错，class后面加个下划线即可
table = page.find('tbody', attrs={"class": "ul"})  # 优化写法
print(table)
# 拿到所有行 ，切片 不取头
trs = table.find("tr")[1:]

# 文件处理
f = open("菜价.csv",mode='w',encoding='utf-8')
csv_write = csv.writer(f)

# 遍历每一行
for tr in trs:
    tds = tr.find("td")  # 拿到每行中所有的td
    name = tds[0].text  # .text 表示拿到被标签标记的内容  菜品名称
    low = tds[1].text  # 最低价
    avg = tds[2].text  # 平均价
    hight = tds[3].text  # 最高价
    guige = tds[4].text  # 规格
    kind = tds[5].text  # 产地
    date = tds[6].text  # 日期
    csv_write.writerow([name,low,avg,hight,guige,kind,date])

print('============== over =============')
```
 

---

# **7.抓取让你睡不着觉的图片**

- 美图库
- from bs4 import BeautifulSoup
- https://www.umei.cc/bizhitupian/weimeibizhi/
- **思路**
    -   1.进入主页面源码，然后提取到子页面的链接地址 ，href
    -   2.通过href拿到子页面的内容，从子页面中找到图片的下载地址 img -> src
    -   3.下载图片

```python
'''

- 美图库
- from bs4 import BeautifulSoup
- https://www.umei.cc/bizhitupian/weimeibizhi/
- **思路**
    -   1.进入主页面源码，然后提取到子页面的链接地址 ，href
    -   2.通过href拿到子页面的内容，从子页面中找到图片的下载地址 img -> src
    -   3.下载图片

'''

import requests
from bs4 import BeautifulSoup
import time

# 1. 获取主页面源码
domain = 'https://www.umei.cc/'
categary = 'bizhitupian/weimeibizhi/'
url = domain + categary

resp = requests.get(url)
resp.encoding = 'utf-8'

# 2.提取数据
# 把源代码交代给bs
main_page = BeautifulSoup(resp.text, "html.parser")
alist = main_page.find("div", attrs={"class": "item_list"}).find_all("a")
# print(alist)
for a in alist:
    href = domain + a.get('href').strip('/')  # 直接通过get就可以拿到属性的值

    # 拿到子页面的源代码
    child_page_resp = requests.get(href)
    child_page_resp.encoding = 'utf-8'
    child_page_text = child_page_resp.text

    # 从子页面中拿到图片的下载路径
    child_page = BeautifulSoup(child_page_text, "html.parser")
    img_src = child_page.find("div", class_="big-pic").find("img").get("src")
    # print(img_src)

    # 3.下载图片
    img_resp = requests.get(img_src)
    # 获取图片保存的名称 ,src的最后结果的字符串命名
    img_floder = '美图库文件/'
    img_name = img_src.split('/')[-1]  # 拿到src中最后一个/以后的内容
    with open(img_floder + img_name, mode='wb') as f:
        f.write(img_resp.content)  # 图片写入到文件

    print("over! 图片命名：", img_name)
    # break  # 测试单次

    time.sleep(1)  # 暂停1秒

print("==== all over ====")

```
---

# **8.xpath解析**

---

# **9.抓取猪八戒数据**
